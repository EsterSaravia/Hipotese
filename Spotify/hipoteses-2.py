# -*- coding: utf-8 -*-
"""Hipoteses.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13bwPleR17e9vIA3LvN1EaiRzcyCnl4qj

**Introdução** Neste estudo, vamos a  determinar quais variáveis têm influência no número de streams e se a popularidade de artistas no Spotify está relacionada à popularidade em outros serviços de streaming, vamos usar técnicas de regressão, como regressão linear múltipla para prever o número de streams com base nas variáveis independentes e análise de correlação para examinar as relações entre as popularidades nos diferentes serviços de streaming..
"""

# importar pandas e tabelas
import pandas as pd
spotify = pd.read_csv('/content/track_in_spotify - spotify.csv')
competition = pd.read_csv('/content/track_in_competition - competition.csv')
technical = pd.read_csv('/content/track_technical_info - technical_info.csv')

# Unir os DataFrames usando o track_id como chave de junção
merged = pd.merge(spotify, competition, on='track_id')
merged = pd.merge(merged, technical, on='track_id')

# Remover linhas com valores não numéricos na coluna streams
merged = merged[pd.to_numeric(merged['streams'], errors='coerce').notnull()]

# Converter a coluna streams para int64
merged['streams'] = merged['streams'].astype(int)

# Converter a coluna deezer playlists para int64
merged['in_deezer_playlists'] = merged['in_deezer_playlists'].str.replace(',', '').astype(int)
# Verificar se a coluna deezer playlists foi convertida com sucesso
print(merged.info())

# Somar as colunas de playlists
merged['total_playlists'] = merged['in_spotify_playlists'] + merged['in_apple_playlists'] + merged['in_deezer_playlists']

# Verificar se a nova coluna foi criada corretamente
print(merged.info())

# Remover colunas foro escopo
merged = merged.drop(['released_year', 'released_month', 'released_day', 'in_spotify_playlists', 'in_deezer_playlists', 'in_apple_playlists', 'key', 'mode', 'in_shazam_charts'], axis=1)

# Verificar se as colunas foi eliminada com sucesso
print(merged.info())

"""Para a regressão linear ou multilinear, não é necessário que os dados sigam uma **distribuição normal**. A suposição de normalidade se aplica aos **resíduos do modelo**, não aos dados originais. Ou seja, os resíduos devem ter uma distribuição normal para que as estimativas dos parâmetros do modelo sejam não tendenciosas e eficientes."""

#importa o modelo regressao linear
import statsmodels.api as sm
from sklearn.preprocessing import StandardScaler

# Hipótese 1 - Músicas com BPM mais altos fazem mais sucesso
X = merged['bpm']
Y = merged['streams']
X = sm.add_constant(X)
model = sm.OLS(Y, X).fit()
# Calcular os resíduos
residuos_hipotese_1 = model.resid
print("Hipótese 1 - Músicas com BPM mais altos fazem mais sucesso:")
print(model.summary())
print('\nResposta: ', residuos_hipotese_1)

# Hipótese 2 - Comparação entre Spotify e Deezer
X = merged['in_deezer_charts']
Y = merged['in_spotify_charts']
X = sm.add_constant(X)
model = sm.OLS(Y, X).fit()
# Calcular os resíduos
residuos_hipotese_deezer = model.resid
print("\nHipótese 2 - Comparação entre Spotify e Deezer:")
print(model.summary())
print('\nResposta: ', residuos_hipotese_deezer)
# Hipótese 2 - Comparação entre Spotify e Apple
X = merged['in_apple_charts']
Y = merged['in_spotify_charts']
X = sm.add_constant(X)
model = sm.OLS(Y, X).fit()
# Calcular os resíduos
residuos_hipotese_apple = model.resid
print("\nHipótese 2 - Comparação entre Spotify e Apple:")
print(model.summary())

# Hipótese 3 - Correlação entre playlists e streams
X = merged['total_playlists']
Y = merged['streams']
X = sm.add_constant(X)
model = sm.OLS(Y, X).fit()
# Calcular os resíduos
residuos_hipotese_3 = model.resid
print("\nHipótese 3 - Correlação entre playlists e streams:")
print(model.summary())

#hipótese 4 -  Artistas com um maior número de músicas têm mais streams
X = merged['artist_count']
Y = merged['streams']
X = sm.add_constant(X)
model = sm.OLS(Y, X).fit()
# Calcular os resíduos
residuos_hipotese_4 = model.resid
print("\nHipótese 4 - Artistas com um maior número de músicas têm mais streams:")
print(model.summary())

# Hipótese 5 - Influência da danceability no número de streams
X = merged['danceability_%']
Y = merged['streams']
X = sm.add_constant(X)
model = sm.OLS(Y, X).fit()
# Calcular os resíduos
residuos_hipotese_dance = model.resid
print("\nHipótese 5 - Influência da danceability no número de streams:")
print(model.summary())
# Hipótese 5 - Influência da energy no número de streams
X = merged['energy_%']
Y = merged['streams']
X = sm.add_constant(X)
model = sm.OLS(Y, X).fit()
# Calcular os resíduos
residuos_hipotese_energy = model.resid
print("\nHipótese 5 - Influência da energy no número de streams:")
print(model.summary())
# Hipótese 5 - Influência da valence no número de streams
X = merged['valence_%']
Y = merged['streams']
X = sm.add_constant(X)
model = sm.OLS(Y, X).fit()
# Calcular os resíduos
residuos_hipotese_valence = model.resid
print("\nHipótese 5 - Influência da valence no número de streams:")
print(model.summary())
# Hipótese 5 - Influência da liveness no número de streams
X = merged['liveness_%']
Y = merged['streams']
X = sm.add_constant(X)
model = sm.OLS(Y, X).fit()
# Calcular os resíduos
residuos_hipotese_liveness = model.resid
print("\nHipótese 5 - Influência da liveness no número de streams:")
print(model.summary())
# Hipótese 5 - Influência da instrumentalness no número de streams
X = merged['instrumentalness_%']
Y = merged['streams']
X = sm.add_constant(X)
model = sm.OLS(Y, X).fit()
# Calcular os resíduos
residuos_hipotese_instrumental = model.resid
print("\nHipótese 5 - Influência da instrumentalness no número de streams:")
print(model.summary())
# Hipótese 5 - Influência da acousticness no número de streams
X = merged['acousticness_%']
Y = merged['streams']
X = sm.add_constant(X)
model = sm.OLS(Y, X).fit()
# Calcular os resíduos
residuos_hipotese_acoutic = model.resid
print("\nHipótese 5 - Influência da acousticness no número de streams:")
print(model.summary())
# Hipótese 5 - Influência da speechiness no número de streams
X = merged['speechiness_%']
Y = merged['streams']
X = sm.add_constant(X)
model = sm.OLS(Y, X).fit()
# Calcular os resíduos
residuos_hipotese_speech = model.resid
print("\nHipótese 5 - Influência da speechiness no número de streams:")
print(model.summary())

from scipy.stats import shapiro

# Teste de Shapiro-Wilk
stat, p_valor = shapiro(residuos_hipotese_1)
# Verificar o resultado
if p_valor > 0.05:
    print("Os resíduos da Hipótese 1 parecem seguir uma distribuição normal.")
else:
    print("Os resíduos da Hipótese 1 não seguem uma distribuição normal.")

# Teste de Shapiro-Wilk
stat, p_valor = shapiro(residuos_hipotese_deezer)
# Verificar o resultado
if p_valor > 0.05:
    print("Os resíduos da Hipótese deezer parecem seguir uma distribuição normal.")
else:
    print("Os resíduos da Hipótese deezer não seguem uma distribuição normal.")

# Teste de Shapiro-Wilk
stat, p_valor = shapiro(residuos_hipotese_apple)
# Verificar o resultado
if p_valor > 0.05:
    print("Os resíduos da Hipótese apple parecem seguir uma distribuição normal.")
else:
    print("Os resíduos da Hipótese apple não seguem uma distribuição normal.")

# Teste de Shapiro-Wilk
stat, p_valor = shapiro(residuos_hipotese_3)
# Verificar o resultado
if p_valor > 0.05:
    print("Os resíduos da Hipótese 3 parecem seguir uma distribuição normal.")
else:
    print("Os resíduos da Hipótese 3 não seguem uma distribuição normal.")

# Teste de Shapiro-Wilk
stat, p_valor = shapiro(residuos_hipotese_4)
# Verificar o resultado
if p_valor > 0.05:
    print("Os resíduos da Hipótese 4 parecem seguir uma distribuição normal.")
else:
    print("Os resíduos da Hipótese 4 não seguem uma distribuição normal.")

"""**A Regressão Linear Múltipla** é um modelo de análise que usamos quando modelamos a relação linear entre uma variável de desfecho(dependente) contínua e múltiplas variáveis preditoras(independente) que podem ser contínuas ou categóricas."""

# Importando as bibliotecas
from sklearn.linear_model import LinearRegression
from scipy.stats import pearsonr

# os dados, onde X é independente e Y dependente
X = merged[['total_playlists', 'in_spotify_charts', 'bpm', 'danceability_%', 'energy_%', 'instrumentalness_%', 'valence_%', 'acousticness_%', 'liveness_%', 'speechiness_%', 'in_deezer_charts', 'in_apple_charts']]
y = merged['streams']

# Criando e treinando o modelo de Regressão Linear Múltipla
model_multiplo = LinearRegression()
model_multiplo.fit(X, y)

# Coeficientes do modelo
coeficientes = pd.DataFrame(model_multiplo.coef_, X.columns, columns=['Coeficiente'])
print("Coeficientes do modelo linear multiplo:")
print(coeficientes)

# Calculando a correlação entre as variáveis
correlacao_spotify_deezer, _ = pearsonr(merged['in_spotify_charts'], merged['in_deezer_charts'])
correlacao_spotify_apple, _ = pearsonr(merged['in_spotify_charts'], merged['in_apple_charts'])
print(f"\nCorrelação entre popularidade no Spotify e no Deezer: {correlacao_spotify_deezer}")
print(f"\nCorrelação entre popularidade no Spotify e no Apple Music: {correlacao_spotify_apple}")

from scipy.stats import shapiro

# Calculando os resíduos do modelo
merged['residuos'] = y - model_multiplo.predict(X)

# Teste de Shapiro-Wilk para verificar a normalidade dos resíduos
stat, p_valor = shapiro(merged['residuos'])

# Verificando o resultado do teste
if p_valor > 0.05:
    print("Os resíduos parecem ser normalmente distribuídos.")
else:
    print("Os resíduos não parecem ser normalmente distribuídos.")

"""**Quando os resíduos do seu modelo de Regressão Linear Múltipla não têm uma distribuição normal, há algumas opções que você pode considerar:**
1- **Transformações nos dados:** Experimente transformar suas variáveis ou a variável de resposta para tornar os resíduos mais normalmente distribuídos. Por exemplo, você pode tentar transformações logarítmicas ou raiz quadrada.
2- **Modelos alternativos:** Considere o uso de modelos diferentes que sejam mais adequados para seus dados. Por exemplo, modelos de regressão robusta podem ser mais apropriados para dados com resíduos não normalmente distribuídos.
3- **Considerar outras variáveis:** Pode haver variáveis importantes que não foram incluídas no modelo e que podem ajudar a explicar a variabilidade nos dados.
4- **Validação do modelo:** Verifique se o modelo está se ajustando bem aos dados de treinamento e se é capaz de fazer previsões precisas em dados de teste.
5- **Análise adicional:** Realize uma análise mais aprofundada dos resíduos, como a análise de resíduos versus previsões ou resíduos versus variáveis independentes, para identificar padrões ou tendências que possam indicar problemas com o modelo.
"""

# Tranformaçao nos dados
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression
from scipy.stats import shapiro

# Normalizando os dados
scaler_X = StandardScaler()
X_scaled = scaler_X.fit_transform(X)

scaler_y = StandardScaler()
y_scaled = scaler_y.fit_transform(y.values.reshape(-1, 1))

# Criando e treinando o modelo de Regressão Linear Múltipla com dados normalizados
model_multiplo = LinearRegression()
model_multiplo.fit(X_scaled, y_scaled)

# Coeficientes do modelo
coeficientes = pd.DataFrame(model_multiplo.coef_.reshape(-1, 1), X.columns, columns=['Coeficiente'])
print("Coeficientes do modelo:")
print(coeficientes)

# Verificando a distribuição dos resíduos
residuos = y_scaled - model_multiplo.predict(X_scaled)
stat, p_valor = shapiro(residuos)
if p_valor > 0.05:
    print("\nOs resíduos parecem ser normalmente distribuídos após a normalização dos dados.")
else:
    print("\nOs resíduos ainda não parecem ser normalmente distribuídos após a normalização dos dados.")

# Modelos alternativos: modelo de regressao robusta
from sklearn.linear_model import HuberRegressor
from sklearn.preprocessing import StandardScaler
from scipy.stats import shapiro

# Normalizando os dados
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

scaler_y = StandardScaler()
y_scaled = scaler_y.fit_transform(y.values.reshape(-1, 1))

# Criando e ajustando o modelo de regressão robusta
model_robusto = HuberRegressor()
model_robusto.fit(X_scaled, y)

# Coeficientes do modelo robusto
coeficientes_robustos = pd.DataFrame(model_robusto.coef_.reshape(-1, 1), X.columns, columns=['Coeficiente Robusto'])
print("\nCoeficientes do modelo robusto:")
print(coeficientes_robustos)

# Calculando os resíduos do modelo
residuos = y - model_robusto.predict(X_scaled)

# Teste de Shapiro-Wilk para verificar a normalidade dos resíduos
stat, p_valor = shapiro(residuos)

# Verificando o resultado do teste
if p_valor > 0.05:
    print("\nOs resíduos parecem ser normalmente distribuídos.")
else:
    print("\nOs resíduos não parecem ser normalmente distribuídos.")

"""Quanto à **normalidade dos resíduos**, o teste de Shapiro-Wilk sugere que os resíduos **não seguem uma distribuição normal**, mesmo após a normalização dos dados. Isso pode indicar que o **modelo não captura** completamente a relação entre as variáveis independentes e dependentes, ou pode haver **outras variáveis importantes não incluídas no modelo**.

Nesse caso, pode ser útil explorar outras técnicas de modelagem(fiz robusta) ou considerar a inclusão de outras variáveis(nao tenho) no modelo para melhorar a distribuição dos resíduos.
"""

#Validaçao do Modelo
#importar biblio
from sklearn.metrics import mean_absolute_error
import numpy as np

# Calculando os resíduos
residuos = y_scaled - model_multiplo.predict(X_scaled)

# Calculando as métricas
mae = mean_absolute_error(y_scaled, model_multiplo.predict(X_scaled))
mape = np.mean(np.abs(residuos) / np.abs(y_scaled)) * 100
median_residuos = np.median(residuos)

print(f"Erro Absoluto Médio (MAE): {mae}") #o modelo erra cerca de 0.3879 unidades ao prever os dados normalizados.
print(f"Erro Percentual Absoluto Médio (MAPE): {mape}") #19.36%. Isso indica um erro percentual considerável, mostrando que o modelo tem dificuldade em prever com precisão os dados normalizados.
print(f"Mediana dos Resíduos: {median_residuos}") #-0.0991. Isso significa que a metade dos resíduos é maior que esse valor e a outra metade é menor.

#Validaçao do Modelo
#importar biblio
from sklearn.metrics import mean_absolute_error
import numpy as np

# Calculando os resíduos
residuos = y_scaled - model_robusto.predict(X_scaled)

# Calculando as métricas
mae = mean_absolute_error(y_scaled, model_robusto.predict(X_scaled))
mape = np.mean(np.abs(residuos) / np.abs(y_scaled)) * 100
median_residuos = np.median(residuos)

print(f"Erro Absoluto Médio (MAE): {mae}") #Isso significa que, em média, o modelo erra cerca de 347.8 milhões de unidades ao prever o número de streams.
print(f"Erro Percentual Absoluto Médio (MAPE): {mape}") #174400888118.3%, o que indica um erro percentual extremamente alto. Isso sugere que o modelo tem dificuldade em prever os dados com precisão.
print(f"Mediana dos Resíduos: {median_residuos}") #-345.1 milhões. Isso significa que a metade dos resíduos é maior que esse valor e a outra metade é menor

#importa o modelo
import statsmodels.api as sm
from sklearn.preprocessing import StandardScaler

# Hipótese 1 - Músicas com BPM mais altos fazem mais sucesso
X = merged[['bpm']]
Y = merged['streams']
# Normalizar os dados
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
# Adiciona uma constante para o termo independente
X_scaled = sm.add_constant(X_scaled)
model = sm.OLS(Y, X_scaled).fit()
# Calcular os resíduos
residuos_hipotese_1 = model.resid
print("Hipótese 1 - Músicas com BPM mais altos fazem mais sucesso:")
print(model.summary())
print('\nResposta: ', residuos_hipotese_1)

# Hipótese 2 - Comparação entre Spotify e Deezer
X = merged[['in_deezer_charts']]
Y = merged['in_spotify_charts']
# Normalizar os dados
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
# Adiciona uma constante para o termo independente
X_scaled = sm.add_constant(X_scaled)
model = sm.OLS(Y, X_scaled).fit()
# Calcular os resíduos
residuos_hipotese_deezer = model.resid
print("\nHipótese 2 - Comparação entre Spotify e Deezer:")
print(model.summary())
print('\nResposta: ', residuos_hipotese_deezer)
# Hipótese 2 - Comparação entre Spotify e Apple
X = merged[['in_apple_charts']]
Y = merged['in_spotify_charts']
# Normalizar os dados
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
# Adiciona uma constante para o termo independente
X_scaled = sm.add_constant(X_scaled)
model = sm.OLS(Y, X_scaled).fit()
# Calcular os resíduos
residuos_hipotese_apple = model.resid
print("\nHipótese 2 - Comparação entre Spotify e Apple:")
print(model.summary())

# Hipótese 3 - Correlação entre playlists e streams
X = merged[['total_playlists']]
Y = merged['streams']
# Normalizar os dados
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
# Adiciona uma constante para o termo independente
X_scaled = sm.add_constant(X_scaled)
model = sm.OLS(Y, X_scaled).fit()
# Calcular os resíduos
residuos_hipotese_3 = model.resid
print("\nHipótese 3 - Correlação entre playlists e streams:")
print(model.summary())

#hipótese 4 -  Artistas com um maior número de músicas têm mais streams
X = merged[['artist_count']]
Y = merged['streams']
# Normalizar os dados
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
# Adiciona uma constante para o termo independente
X_scaled = sm.add_constant(X_scaled)
model = sm.OLS(Y, X_scaled).fit()
# Calcular os resíduos
residuos_hipotese_4 = model.resid
print("\nHipótese 4 - Artistas com um maior número de músicas têm mais streams:")
print(model.summary())

# Hipótese 5 - Influência da danceability no número de streams
X = merged[['danceability_%']]
Y = merged['streams']
# Normalizar os dados
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
# Adiciona uma constante para o termo independente
X_scaled = sm.add_constant(X_scaled)
model = sm.OLS(Y, X_scaled).fit()
# Calcular os resíduos
residuos_hipotese_dance = model.resid
print("\nHipótese 5 - Influência da danceability no número de streams:")
print(model.summary())

from scipy.stats import shapiro

# Realizar o teste de Shapiro-Wilk para verificar a normalidade dos resíduos
shapiro_test_h1 = shapiro(residuos_hipotese_1)
shapiro_test_deezer = shapiro(residuos_hipotese_deezer)
shapiro_test_apple = shapiro(residuos_hipotese_apple)
shapiro_test_3 = shapiro(residuos_hipotese_3)
shapiro_test_4 = shapiro(residuos_hipotese_4)
shapiro_test_dance = shapiro(residuos_hipotese_dance)

# Exibir os resultados dos testes de normalidade
print("Teste de Shapiro-Wilk para normalidade dos resíduos:")
print("\nHipótese 1: ", shapiro_test_h1)
print("\nHipótese 2 (Deezer):", shapiro_test_deezer)
print("\nHipótese 2 (Apple):", shapiro_test_apple)
print("\nHipótese 3:", shapiro_test_3)
print("\nHipótese 4:", shapiro_test_4)
print("\nHipótese 5 (Danceability):", shapiro_test_dance)

#Validaçao do Modelo
#importar biblio
from sklearn.metrics import mean_absolute_error
import numpy as np

# Calculando os resíduos
residuos = y_scaled - model.predict(X_scaled)

# Calculando as métricas
mae = mean_absolute_error(y_scaled, model.predict(X_scaled))
mape = np.mean(np.abs(residuos) / np.abs(y_scaled)) * 100
median_residuos = np.median(residuos)

print(f"Erro Absoluto Médio (MAE): {mae}") #514.1 milhões. Isso significa que, em média, o modelo erra cerca de 514.1 milhões de unidades ao prever o número de streams.
print(f"Erro Percentual Absoluto Médio (MAPE): {mape}") #aproximadamente 257810138419.3%, o que indica um erro percentual extremamente alto. Isso sugere que o modelo tem dificuldade em prever os dados com precisão.
print(f"Mediana dos Resíduos: {median_residuos}") #-505.9 milhões. Isso significa que a metade dos resíduos é maior que esse valor e a outra metade é menor.

"""Os **resultados** dos testes indicam que os modelos **pode não estar performando bem na previsão dos dados**. O MAE e o MAPE indicam que há uma diferença significativa entre os valores previstos e os valores reais. Além disso, a mediana dos resíduos sugere que o modelo pode estar subestimando os valores em geral. Portanto, seria recomendável investigar e possivelmente ajustar o modelo para melhorar sua precisão.

**O teste de Wald** é comumente usado para testar a **significância de um coeficiente em um modelo de regressão**. Ele não determina **causalidade diretamente**, mas pode indicar se há uma **relação significativa entre uma variável independente e a variável dependente no modelo**. A causalidade requer mais do que simplesmente testar a significância de um coeficiente; geralmente requer uma análise cuidadosa, incluindo a consideração de outros fatores e possíveis viéses.
"""

#Não é necessário normalizar os dados para realizar os testes de Wald, Granger
#ou Sims. Esses testes são baseados nos coeficientes estimados dos modelos e nas propriedades estatísticas dos resíduos, não nos valores brutos das variáveis. (pode realizar esses testes com os dados originais).

import pandas as pd
import statsmodels.api as sm
from scipy.stats import chi2

# Seus dados
X = merged[['total_playlists', 'in_spotify_charts', 'bpm', 'danceability_%']]
y = merged['streams']

# Adiciona uma constante para o termo independente
X = sm.add_constant(X)

# Ajusta o modelo
model_wald = sm.OLS(y, X).fit()

# Matriz de restrição para testar todos os coeficientes simultaneamente
r_matrix = np.eye(len(model_wald.params))

# Realiza o teste de Wald
wald_test = model_wald.wald_test(r_matrix)

# Obtém o valor crítico para o teste qui-quadrado
alpha = 0.05
df = len(model_wald.params)  # Graus de liberdade
critical_value = chi2.ppf(1 - alpha, df)

# Imprime o resultado do teste
print('Valor do teste de Wald:', wald_test.statistic[0][0])
print('Valor crítico:', critical_value)
print('P-valor:', wald_test.pvalue)
print('Os coeficientes são significativos?', wald_test.statistic[0][0] > critical_value)

"""Utilizar **modelos de séries temporais** para analisar a relação entre as variáveis ao longo do tempo e identificar possíveis relações causais."""

import numpy as np
import pandas as pd
import statsmodels.api as sm
from sklearn.preprocessing import StandardScaler
from scipy.stats import shapiro

X = merged[['total_playlists', 'in_spotify_charts', 'bpm', 'danceability_%']]
Y = merged['streams']

# Normalizar os dados
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Criando o modelo de série temporal com os dados normalizados
model_tempo = sm.tsa.ARIMA(endog=Y, exog=X_scaled, order=(1, 0, 0))
results = model_tempo.fit()

# Calculando as métricas
log_likelihood = results.llf
aic = results.aic
bic = results.bic
mae = np.mean(np.abs(results.resid))
mape = np.mean(np.abs(results.resid) / np.abs(Y))
ljung_box = sm.stats.acorr_ljungbox(results.resid, lags=[10], return_df=True).iloc[0, 1]
jarque_bera = sm.stats.jarque_bera(results.resid)[0]
shapiro_test = shapiro(results.resid)

# Exibindo as métricas
print(f"Log Likelihood: {log_likelihood}") #medida da adequação do modelo aos dados. Quanto maior, melhor.
print(f"\nAIC: {aic}") #medida da qualidade relativa de um modelo estatístico, levando em conta a complexidade do modelo. Quanto menor, melhor.
print(f"\nBIC: {bic}") #É similar ao AIC, mas penaliza modelos com mais parâmetros de forma mais severa. Quanto menor, melhor.
print(f"\nMAE: {mae}") #É a média dos valores absolutos dos resíduos. Quanto menor, melhor.
print(f"\nMAPE: {mape}") #É a média da porcentagem dos valores absolutos dos resíduos em relação aos valores reais. Quanto menor, melhor.
print(f"\nLjung-Box (10 lags): {ljung_box}") # teste de autocorrelação dos resíduos. Um valor baixo indica que os resíduos não têm autocorrelação.
print(f"\nJarque-Bera: {jarque_bera}") #teste de normalidade dos resíduos. Quanto maior, mais diferente os resíduos são da distribuição normal.
print(f"\nShapiro-Wilk Test (Statistic, p-value): {shapiro_test}")

"""Com base nessas métricas, o modelo ARIMA parece **ter um bom ajuste aos dados**, com **exceção do teste de normalidade dos resíduos**, que sugere que os resíduos **não são normalmente distribuídos**. Isso pode indicar que o modelo não captura completamente a estrutura dos dados ou que há algum padrão nos resíduos que não foi capturado pelo modelo.

O **teste de causalidade de Granger** é uma ferramenta útil para investigar a relação de causa e efeito entre variáveis em séries temporais, ajudando a identificar quais variáveis são importantes para prever o comportamento de outras variáveis ao longo do tempo.
"""

# Importar o teste de causalidade de Granger
from statsmodels.tsa.stattools import grangercausalitytests

# Selecionar as variáveis para o teste de causalidade de Granger
data = merged[['total_playlists', 'streams']]

# Definir o número máximo de lags a serem considerados
max_lag = 3

# Executar o teste de causalidade de Granger
results_granger = grangercausalitytests(data, max_lag, verbose=True)

# Verificar se o valor-p é menor que 0.05
for lag in range(1, max_lag + 1):
    p_value = results_granger[lag][0]['ssr_ftest'][1]
    if p_value < 0.05:
        print(f'O teste de causalidade de Granger para lag {lag} é significativo (p-valor = {p_value}).')
    else:
        print(f'O teste de causalidade de Granger para lag {lag} não é significativo (p-valor = {p_value}).')

# Exibir os resultados completos do teste de causalidade de Granger
print('\nResultados completos do teste de Causalidade de Granger:')
print(results_granger)

from statsmodels.tsa.stattools import grangercausalitytests

# Preparar os dados
data = merged[['in_spotify_charts', 'streams']]
max_lag = 3  # Defina o número máximo de lags a serem considerados

# Executar o teste de causalidade de Granger
results_granger = grangercausalitytests(data, max_lag, verbose=True)

# Avaliar os resultados
for lag in results_granger.keys():
    causality = results_granger[lag][0]['ssr_ftest'][1]
    if causality < 0.05:  # Verifique se a causalidade é estatisticamente significativa
        print(f'O teste de causalidade de Granger para lag {lag} é significativo (p-valor = {p_value}).')
    else:
        print(f'O teste de causalidade de Granger para lag {lag} não é significativo (p-valor = {p_value}).')

# Exibir os resultados completos do teste de causalidade de Granger
print('\nResultados completos do teste de Causalidade de Granger:')
print(results_granger)

from statsmodels.tsa.stattools import grangercausalitytests

# Preparar os dados
data = merged[['bpm', 'streams']]
# Executar o teste de causalidade de Granger
max_lag = 3  # Defina o número máximo de lags a serem considerados
results_granger = grangercausalitytests(data, max_lag, verbose=True)

# Avaliar os resultados
for lag in results_granger.keys():
    causality = results_granger[lag][0]['ssr_ftest'][1]
    if causality < 0.05:  # Verifique se a causalidade é estatisticamente significativa
       print(f'O teste de causalidade de Granger para lag {lag} é significativo (p-valor = {p_value}).')
    else:
        print(f'O teste de causalidade de Granger para lag {lag} não é significativo (p-valor = {p_value}).')

# Exibir os resultados completos do teste de causalidade de Granger
print('\nResultados completos do teste de Causalidade de Granger:')
print(results_granger)

from statsmodels.tsa.stattools import grangercausalitytests

# Preparar os dados
data = merged[['danceability_%', 'streams']]
# Executar o teste de causalidade de Granger
max_lag = 3  # Defina o número máximo de lags a serem considerados
results_granger = grangercausalitytests(data, max_lag, verbose=True)

# Avaliar os resultados
for lag in results_granger.keys():
    causality = results_granger[lag][0]['ssr_ftest'][1]
    if causality < 0.05:  # Verifique se a causalidade é estatisticamente significativa
       print(f'O teste de causalidade de Granger para lag {lag} é significativo (p-valor = {p_value}).')
    else:
        print(f'O teste de causalidade de Granger para lag {lag} não é significativo (p-valor = {p_value}).')

# Exibir os resultados completos do teste de causalidade de Granger
print('\nResultados completos do teste de Causalidade de Granger:')
print(results_granger)

from statsmodels.tsa.stattools import grangercausalitytests

# Preparar os dados
data = merged[['liveness_%', 'streams']]
# Executar o teste de causalidade de Granger
max_lag = 3  # Defina o número máximo de lags a serem considerados
results_granger = grangercausalitytests(data, max_lag, verbose=True)

# Avaliar os resultados
for lag in results_granger.keys():
    causality = results_granger[lag][0]['ssr_ftest'][1]
    if causality < 0.05:  # Verifique se a causalidade é estatisticamente significativa
       print(f'O teste de causalidade de Granger para lag {lag} é significativo (p-valor = {p_value}).')
    else:
        print(f'O teste de causalidade de Granger para lag {lag} não é significativo (p-valor = {p_value}).')

# Exibir os resultados completos do teste de causalidade de Granger
print('\nResultados completos do teste de Causalidade de Granger:')
print(results_granger)

from statsmodels.tsa.stattools import grangercausalitytests

# Preparar os dados
data = merged[['speechiness_%', 'streams']]
# Executar o teste de causalidade de Granger
max_lag = 3  # Defina o número máximo de lags a serem considerados
results_granger = grangercausalitytests(data, max_lag, verbose=True)

# Avaliar os resultados
for lag in results_granger.keys():
    causality = results_granger[lag][0]['ssr_ftest'][1]
    if causality < 0.05:  # Verifique se a causalidade é estatisticamente significativa
       print(f'O teste de causalidade de Granger para lag {lag} é significativo (p-valor = {p_value}).')
    else:
        print(f'O teste de causalidade de Granger para lag {lag} não é significativo (p-valor = {p_value}).')

# Exibir os resultados completos do teste de causalidade de Granger
print('\nResultados completos do teste de Causalidade de Granger:')
print(results_granger)

"""Com base nos resultados apresentados, **há evidência de causalidade de Granger entre as variáveis**(popularidade e playlists). Isso significa que **uma variável é útil na previsão da outra variável**, indicando uma relação causal entre elas."""